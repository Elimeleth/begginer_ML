# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WKjlfi0XnUgD6QOPuphP0NFzYImuWbra
"""

# USAMOS LA FUNCIONALIDAD DE OFRECE COLAB DE CARGAR ARCHIVOS
from google.colab import files
files.upload_file('recetas.csv')

# INSTALAMOS LA LIBRERIA TRANSFORMERS
!pip install transformers

from sklearn.model_selection import train_test_split # VALIDACION Y TEST SPLITEADOS EN UN PORCENTAJE

import re # LIBRERIA PARA TRABAJAR CON EXPRESIONES REGULARES
import pandas as pd # IMPORTAMOS PANDAS DADO QUE CONTAMOS CON UN DATASET EN CSV Y NUESTRO SEPARADOR ES "|"

recetas = pd.read_csv('/content/recetas.csv', sep='|', skip_blank_lines=True)

ingredientes = recetas[['Ingredientes']] # DE AQUI OBTENEMOS NUESTRA VARIABLE TARGET
ingredientes.rename({'Ingredientes':'ingredientes'}, inplace=True) # RENOMBRAMOS NUESTRA VARIABLE

ingredientes.head() # VISUALIZAMOS COMO SE MUESTRA LA DATA

data = ingredientes['Ingredientes'].astype('str').apply(lambda x: re.sub(r'[^a-zA-Z0-9]+', ' ', x)) # APLICAMOS UN LIMPIADO A NUESTRO DATASET

data.head()

train, test = train_test_split(recetas,test_size=0.15) # TOMAMOS EL 15% DE NUESTRA DATA PARA TEST

data = recetas["Ingredientes"].values
print(data)

# CON ESTA FUNCIONALIDAD PRETENDEMOS CREAR UN ARCHIVO DE TRAIN Y TEST CON VECTORES QUE INDIQUEN LOS TEXTOS TOKENIZADOS
def build_text_files(recetas, destino):
    f = open(destino, 'w')
    data = ''
    for ingredientes in recetas["Ingredientes"].values:
        try:
          summary = str(ingredientes).strip()
          summary = re.sub(r"\s", " ", summary)
          data += summary + "  "
        except:
          pass
    f.write(data)
    return data

build_text_files(train,'/content/train_dataset.txt')
build_text_files(test,'/content/test_dataset.txt')

print("Train dataset length: "+str(len(train))) # VISUALIZAMOS LA LONGITUD DE NUESTRO DATASET
print("Test dataset length: "+ str(len(test)))

from transformers import AutoTokenizer # IMPORTAMOS NUESTRO TOKENIZER BASANDONOS EN UN MODELO ESPANYOL EN GPT2

tokenizer = AutoTokenizer.from_pretrained("DeepESP/gpt2-spanish")

train_path = '/content/train_dataset.txt'
test_path = '/content/test_dataset.txt'

from transformers import TextDataset,DataCollatorForLanguageModeling # IMPORTAMOS NUESTRA UTILERIA PARA TRABAJAR CON TEXTOS Y PASARLOS A VECTORES

def load_dataset(train_path,test_path,tokenizer):
    train_dataset = TextDataset(
          tokenizer=tokenizer,
          file_path=train_path,
        overwrite_cache=True,
          block_size=128)

    test_dataset = TextDataset(
          tokenizer=tokenizer,
          file_path=test_path,

        overwrite_cache=True,
          block_size=128)

    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=False,
    )
    return train_dataset,test_dataset,data_collator

train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer) # LLAMAMOS A NUESTRA FUNCION

test_dataset.examples

from transformers import Trainer, TrainingArguments, AutoModelWithLMHead # IMPORTAMOS EL ENTRENADOR, EL BUILD DE ARGUMENTOS Y EL MODELO CON MULTIPLE LAYERS HEADS

model = AutoModelWithLMHead.from_pretrained("DeepESP/gpt2-spanish")

# CREAMOS NUESTROS ARGUMENTOS
training_args = TrainingArguments(
    output_dir="./gpt2-recetas", #The output directory
    overwrite_output_dir=True, #overwrite the content of the output directory
    num_train_epochs=3, # number of training epochs
    per_device_train_batch_size=32, # batch size for training
    per_device_eval_batch_size=64,  # batch size for evaluation
    eval_steps = 400, # Number of update steps between two evaluations.
    save_steps=800, # after # steps model is saved
    warmup_steps=500,# number of warmup steps for learning rate scheduler
    )

# CREAMOS NUESTRO ENTRENADOR
trainer = Trainer(
    model=model, #MODELO
    args=training_args, # ARGUMENTOS
    data_collator=data_collator, #ALLOCATOR
    train_dataset=train_dataset, #TRAIN
    eval_dataset=test_dataset, #TEST
    # prediction_loss_only=True,

)

trainer.train() #ENTRENAMOS

trainer.save_model() #GUARDAMOS EL CHECKPOINT

from transformers import pipeline # IMPORTAMOS NUESTRA TUBERIA

# GENERIACION DE TEXTO Y LE PASAMOS NUESTRO MODELO Y EL TOKENIZER QUE USAMOS
chef = pipeline('text-generation',model='./gpt2-recetas', tokenizer='DeepESP/gpt2-spanish',config={'max_length':800})

# LLAMAMOS A NUESTRO MODELO
result = chef('hamburguesa')[0]['generated_text']